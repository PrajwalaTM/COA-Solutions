\begin{flushright}
\textbf{Solutions prepared by Prajwala TM $<$prajwala.tm@gmail.com$>$}
\end{flushright}
\section*{Exercises}
\vskip 1cm

\setcounter{Exercise}{0}
\setcounter{Answer}{0}

\section*{Pipeline Stages}

\begin{ExerciseList}

\Exercise
Show the design of the IF, OF, EX, MA, and RW pipeline stages.
Explain their functionality in detail.
\Answer
\hspace{3mm} \\
1.\textit{IF Stage}:\\
The only additions to the IF Stage is a pipeline register where we save the value of the PC, and contents of the instruction. This is the only information required for the subsequent stages. \\
2.\textit{OF Stage}: \\
The extra additions are the connections to the two pipeline registers IF-OF, and OF-EX. It extracts the fields $rd$,$rs1$,$rs2$, and the immediate from the instruction. These are sent to the register file, the immediate and branch units. We send the contents of the instruction to the control unit that generates the control signals. Only three control signals-\textit{isRet,isImmediate,isSt} are used immediately. The rest of the control signals are used for controlling multiplexers in the subsequent stages of the pipeline. Hence, they are saved in the OF-EX pipeline register. All these control signals are saved and some space is allocated in the instruction packet called $control$. The intermediate results generated also need to be carried. The OF stage generates the \textit{branchTarget}, inputs for the ALU, and the value to be written to memory for a store instruction($op2$). Some fields are allocated in the instruction packet for the same.\\
3.\textit{EX Stage}: \\
The ALU receives its inputs A and B from the OF-EX pipeline register. The results generated by this stage are \textit{aluResult}, the final branch target, and branch outcome. The result is added to the instruction packet, and saved in the EX-MA register. The EX-MA register also contains the rest of the fields namely-PC,contents of the instruction, control signals, and second operand read from the register file
($op2$). \\
4. \textit{MA Stage}: \\
The only operand that the load instruction takes is the result of the ALU, which contains the effective memory address. This is saved in the \textit{aluResult} field of the EX-MA register. The data to be stored resides in the $rd$ register. Here, the $op2$ field is connected to the MDR register, and the $aluResult$ is connected to the MAR register. The control signals-$isLd$ and $isSt$ are also a part of instruction packet, and they are routed to the memory unit. The output of this stage is the result of the load instruction and is saved in the $ldResult$ field of the MA-RW register. \\
5.\textit{RW Stage}: \\
The values of the $aluResult$ and $ldResult$ fields are the inputs this stage requires. These inputs along with next default PC are connected to the multiplexer that chooses the value to be written back. It does not have any pipeline register, since it is the last stage of the pipeline. \\
\Exercise
Why do we need to store the $op2$ field in the instruction packet? Where is it used?
\Answer
The $op2$ field contains the value to be written to memory for a store instruction. The OF stage generates the value of $op2$ by reading contents of the second register. This is thus saved, in the instruction packet at this stage and transferred to the subsequent pipeline stages. It is used in the MA stage, where $op2$ is connected to the MDR register, and is written to the memory address specified by $aluResult$, connected to the MAR. \\
\Exercise
Why is it necessary to have the $control$ field in the instruction packet?
\Answer
In the OF stage, we send the contents of the instruction extracted to the control unit that generates the control signals. Only three control signals-\textit{isRet,isImmediate,isSt} are used immediately. The rest of the control signals are used for controlling multiplexers in the subsequent stages of the pipeline. Hence, they are saved in the OF-EX pipeline register. All these control signals are saved and some space is allocated in the instruction packet called $control$. This is used later in the pipeline. \\
\Exercise
Why do we require latches in a pipeline? Why are edge sensitive latches preferred?
\Answer
We need to design a method that ensures that instructions seamlessly proceed to the subsequent pipeline stage. There is need for a global mechanism that ensures all the instructions proceed to the next stages simultaneously. This can be achieved with the help of a clock. For this purpose, we insert a register between every two consecutive pipeline stages. Each of these are called pipeline latches, or pipeline registers. \\
These are controlled by the falling edge/rising edge of the clock for defining a specific timing such that all the instructions simultaneously proceed to the next stage. Such pipeline latches are called edge-triggered latches. They are preferred over level triggered latches for more accurate results. \\ 
\Exercise
Why is it necessary to split the work in a data path evenly across the pipeline stages?
\Answer
The work in a data path should be evenly split across the pipeline stages to minimise the idleness in the circuit. This ensures that the pipeline is balanced, and the time taken by each stage is approximately the same, and the cycle time can be fixed accordingly. This leads to better performance. \\
\Exercise[difficulty=1]
We know that in an edge sensitive latch, the input signal has to be stable for $t_{hold}$
units of
time after the negative edge. Let us consider a pipeline stage between latches
$L_1$ and $L_2$. Suppose the output of $L_1$ is ready immediately
after the negative edge, and almost
instantaneously reaches the input of $L_2$. In this case, 
we violate the hold time constraint at $L_2$. How can this situation be avoided?
\Answer
The situation can be avoided by adding an additional constraint on the output signal of a latch. The output signal, that is ready should not be transmitted for $t_{hold}$ units of time after the negative edge. In such a case, the output of $L_1$ will not reach $L_2$ for $t_{hold}$ units of time, and the input signal constraint at $L_2$ will not be violated. 
\end{ExerciseList}

\section*{Pipeline Design}

\begin{ExerciseList}
\Exercise
Enumerate the rules for constructing a pipeline diagram.
\Answer
\hspace{3mm} \\
The rules are : \\
1. Construct a grid of cells, which has five rows, and N columns, where N is the total number of clock cycles that we wish to consider. Each of the five rows corresponds to a pipeline stage. \\
2. If an instruction [k] enters the pipeline in cycle $m$, then we add an entry corresponding to [k] in the $m^{th}$ column of the first row. \\
3. In the $(m+1)^{th}$ cycle, the instruction can either stay in the same stage or can move to the next row. We add a corresponding entry in the grid cell. \\
4. In a similar manner, the instruction moves from the IF stage to the RW stage in sequence. It never moves backwards. However, it can stay in the same stage across consecutive cycles. \\
5.We cannot have two entries in a cell. \\
6. We finally remove the instruction from the pipeline diagram after it leaves the RW stage. \\
\Exercise
Describe the different types of hazards in a pipeline.
\Answer
The different types of hazards are : \\ \\
1. \textit{Data hazards}: \\
A data hazard represents the possibilityof erroneous execution because of the anavailability of correct data. Let us consider the following code snippet: 
\begin{Verbatim}
[1]: add r1, r2, r3 
[2]: sub r3, r1, r4 
\end{Verbatim}
Here the add instruction produces the value for register r1 and the sub-instruction uses it as a source operand. Instruction [1] writes the value of r1 in the fifth cycle, and instruction [2] needs to read its value in the third cycle. This is not possible. Such kind of hazard is known as data hazard. There are three types of data hazards - RAW(Read After Write), WAW(Write After Write), WAR(Write After Read). \\ \\
2. \textit{Control hazards}: \\
A control hazard represents the possibility of erroneous execution in a pipeline because instructions in the wrong part of a branch can possibly get executed and save their results in memory, or in the register file.Instructions that would have been executed if a branch would have had an outcome that is different from its real outcome, are said to be on the wrong path. Let us consider the following code snippet: 
\begin{Verbatim}
[1]: beq .foo 
[2]: mov r1,4 
[3]: add r2, r4, r3 
... 
... 
.foo:
[100]: add r4, r1, r2 
\end{Verbatim}
The outcome of the branch is decided in cycle 3, and is communicated to the fetch unit. The fetch unit starts fetching the correct instruction from cycle 4. Now if the branch is taken, then instructions [2] and [3] should not get executed. However, it is not possible to determine the outcome of the branch in cycles 2 and 3. Hence, these instructions will be fetched and will be a part of the pipeline. If the branch is taken, then there is a possibility that instructions [2] and [3] might corrupt the state of the program and introduce an error. These instructions are said to be on the wrong path. This is an example of a control hazard. \\ \\
3. \textit{Structural hazards}: \\
A structural hazard refers to the possibility of instruction not being able to execute because of resource constraints. They arise when multiple instructions try to access a functional unit in the same cycle, and due to capacity limitations, the unit cannot allow all the interested instructions to proceed. In such a case, a few of the instructions in the conflict need to stall their execution. Structural hazards do not arise in \simplerisc pipeline.
\Exercise
In the \simplerisc pipeline, why don't we have structural hazards?
\Answer
In the \simplerisc pipeline, we never have a situation in which multiple instructions across different pipeline stages wish to access the same unit and that unit does not have the capacity to service all the requests. The only units that are accessed by multiple stages are the fetch unit and the register file. The fetchunit is accessed by an instruction in the IF stage, and by branch instructions in EX stage. It is designed to handle both the requests. Likewise, the register file is accessed by instructions in the OF stage, and RW stage. Our register file has two read ports and one write port. It can thus handle both the requests in the same cycle. Thus there is no possibility of a structural hazard in the \simplerisc pipeline. 
\Exercise
Why does a branch have two delay slots in the \simplerisc pipeline?
\Answer
In the \simplerisc pipeline, there need to be a minimum of two instructions between the branch instruction and the instruction at the branch target. This is because we get both the branch outcome and the branch target at the end of the EX stage. At this point, there are two more instructions in the pipeline. These have been fetched when the branch instruction was in the OF and EX stages respectively. They might be potentially on the wrong path. The positions of these two instructions ae known as delay slots. We need two delay slots after a branch because we are not sure about the two subsequent instructions. Either two $nop$ instructions can be inserted or we can possibly find two instructions that execute before the branch instruction and move them to these delay slots. 
\Exercise
What are the \datalock and \branchlock conditions?
\Answer
\hspace{1mm} \\ 
1. \textit{Data-lock}: The data-clock condition states that an instruction cannot leave the OF stage unless it has received the correct data from the register file. This means that the IF and OF stages need to be effectively stalled and the rest of the stages need to be executed till the instruction in the OF stage can safely read its operands. During this time, the instruction that passes from the OF to the EX stage needs to be a $nop$ instruction. \\ \\
2. \textit{Branch-lock} : The branch lock condition states that the instructions on the wrong path should never be executed. Either the processor needs to be stalled till the outcome is known, or techniques to ensure that instructions on the wrong path are not able to commit their changes to memory, or to the registers need to be used .
\Exercise
Write pseudo-code for detecting and handling the branch-lock condition? (without delayed branches) 
\Answer
\hspace{1mm} \\
\begin{Verbatim}
Data : PC of branch instruction p in cycle n 
Result : Converts the instructions to bubbles and returns true if branch is taken, 
else returns false 
Fetch instruction p+4 in cycle n+1
Fetch instruction p+8 in cycle n
Fetch the value of isBranchTaken 
if(isBranchTaken == true) 
{
Instruction in IF stage <- nop
Instruction in OF stage <- nop
return true 
} 
else 
{ 
/* Resume execution */ 
return false
}
\end{Verbatim} 
\Exercise
What is delayed branching?
\Answer
Delayed branching is a technique used to prevent stalls in a pipeline by re-ordering instructions (done by compiler) where the next few 
instructions after a branch instruction are executed irrespective of whether branch is taken or not.

\Exercise[difficulty=1]
Let us consider two designs, $D_1$ and $D_2$. $D_1$ uses a software based approach,
and assumes delayed branching. $D_2$ uses interlocks, and assumes that a branch is
not taken till the outcome is decided. 
Intuitively, which design is faster?
\Answer
The design $D_1$ is faster intuitively. In case of $D_2$, we assume that the branch is not taken till the outcome is decided and we stall for two cycles in the \simplerisc pipeline. In case of $D_1$, we use a smart compiler that reorders the instructions in such a way that data dependencies are not present. The delay slots are filled with instructions which are executed before the branch instruction instead of placing $nop$ instructions. In most cases, intuitively, this takes lesser time than $D_2$ except for a very large set of instructions, since $D_2$ stalls for a minimum of two cycles. Hence, $D_1$ is faster.   
\Exercise
Assume that 20\% of the dynamic instructions executed on a computer are branch
instructions. We use delayed branching is used with one delay slot. Estimate the CPI, if the compiler is
able to fill 85\% of the delay slots. Assume that the base CPI is 1.5. In the base case, we do not
use any delay slot. We stall the pipeline for the total number of delay slots.
\Answer
1.33

\Exercise
Describe the role of the forwarding multiplexers in  each stage of the pipeline.
\Answer
\hspace{1mm} \\
1. \textit{IF Stage} :\\
It does not have any forwarding multiplexer since it does not send or receive any forwarded value. \\ \\
2. \textit{OF Stage}: \\
There are two forwarding multiplexers used in this stage. We need to choose between the first operand read from the register, and the value forwarded from the RW stage. We thus add a multiplexer to help us choose between these two inputs. Likewise, we need to choose between the second operand read from the register, and the value forwarded from the RW stage. To implement   forwarding, we add a multiplexer to make this choice as well. The multiplexer in the baseline design chooses between the second register operand and the immediate computed from the contents of the instruction. \\ \\
3. \textit{EX Stage}: \\
The three inputs that the EX stage gets from the OF stage are A, B(ALU operands), and op2 (second register operand). For A and B we add two multiplexers to choose between the values computed in the OF stage and the values forwarded from the MA and RW stages respectively. For $op2$ field, which possibly contains the store value, we do not need MA$\rightarrow$ EX forwarding. This is because the store value is required in the MA stage, and can be obtained by RW$\rightarrow$MA forwarding. \\ \\
4. \textit{MA Stage}: \\
The memory address is computed in the EX stage, and saved in the \textit{aluResult} field of the instruction packet. The memory unit uses this value directly for the address. In case of a store, the value that needs to be stored $op2$, can possibly be forwarded from the RW stage. Hence, we add a multiplexer which chooses between $op2$ and the value forwarded from the RW stage. \\ \\ 
5. \textit{RW Stage}: \\
Since this is the last stage, it does not use any forwarded value. However, it sends the value that it writes to the register file to the MA,EX, and OF stages respectively. 

\Exercise 
Why do we not require a forwarding path from $MA$ to $EX$ for the $op2$ field?
\Answer
In case of a store, the value to be stored to memory $op2$ is required in the MA stage, and is not required in the $EX$ stage. Hence, this can be obtained using the RW$\rightarrow$MA forwarding path. It thus, reduces the necessity of another forwarding path from $MA$ to $EX$. 
\Exercise Answer the following questions.
\begin{enumerate}[i) ]
\item What are the six possible forwarding paths in our \simplerisc processor? 
\item Which four forwarding paths, are required, and why? (Give examples to support your answer). 
\end{enumerate}
\Answer
\begin{enumerate}[i) ]
\item The six possible forwarding paths in our \simplerisc processor are - RW$\rightarrow$MA, RW$\rightarrow$EX, RW$\rightarrow$OF, MA$\rightarrow$EX, MA$\rightarrow$OF, EX$\rightarrow$OF. \\      
\item We need only the following four forwarding paths - RW$\rightarrow$MA, RW$\rightarrow$EX, RW$\rightarrow$OF, MA$\rightarrow$EX. \\ \\ 
1. \textit{RW$\rightarrow$MA}: \\
  Consider the code snippet: \\
\begin{Verbatim}
[1]: ld r1,4[r2]
[2]: sw r1,10[r3]
\end{Verbatim}
Here, instruction[2] needs the value of $r1$ in the MA stage(cycle 5), and instruction[1] fetches the value of $r1$ from memory by end of cycle 4. Thus, it can forward its value to instruction[2] in cycle 5. \\ \\
2. \textit{RW$\rightarrow$EX}: \\
  Consider the code snippet: \\
\begin{Verbatim}
[1]: ld r1,4[r2]
[2]: sw r8,10[r3]
[3]: add r2,r1,r4
\end{Verbatim}
The load instruction fetches the value of $r1$ by end of cycle 4, and a subsequent ALU instruction requires the value of $r1$ in cycle 5. This forwarding is again possible. \\ \\
3. \textit{MA$\rightarrow$EX}: \\
  Consider the code snippet: \\
\begin{Verbatim}
[1]: add r1,r2,r3
[2]: sub r4,r1,r2
\end{Verbatim}
The ALU instruction computes the value of $r1$ by end of cycle 3, and a consecutive ALU instruction requires the value of $r1$ in cycle 4. This forwarding path makes it possible to forward the required values. \\ \\
4. \textit{RW$\rightarrow$OF}: \\
  Consider the code snippet: \\
\begin{Verbatim}
[1]: ld r1,4[r2]
[2]: sw r4,10[r3]
[3]: sw r5,10[r6]
[4]: sub r7,r1,r2
\end{Verbatim}
Typically, OF stage does not need any forwarding paths since it does not have any functional units. However, forwarding from the RW stage is required since the value cannot be forwarded later(the instruction gets removed from the pipeline).\\ 
In the code snippet, instruction[1] produces the value of $r1$ by reading its value from the memory at the end of cycle 4.It then writes the value of $r1$ to register file in cycle 5. Meanwhile, instruction[4] tries to read the value of $r1$ in OF stage, cycle 5. Hence, this can be obtained by using the forwarding path. \\ \\
The other two forwarding paths MA$\rightarrow$OF, EX$\rightarrow$OF are not required as RW$\rightarrow$EX, MA$\rightarrow$EX forwarding paths can be used instead. \\
\end{enumerate}

\Exercise Assume that we have an instruction immediately after a call instruction that reads $ra$. We claim 
that this instruction will get the correct value of $ra$ in a pipeline with forwarding. Is this true? Prove your answer.
\Answer
Consider a $ret$ instruction immediately after a $call$, since it reads the value of $ra$. This instruction gets the correct value of $ra$.
A $call$ instruction is a taken branch. This means that when it enters the EX stage, the $Branch-Lock$ circuitry will detect that it is a taken branch, and convert the instructions in the IF and OF stages to bubbles. Any instruction that requires the value of $ra$ will atleast be three stages behind the $call$ instruction. This means that when the $call$ instruction reaches the RW stage, the next valid instruction in the pipeline will be in the OF stage. If this requires the value of $ra$, it can get it using the RW$\rightarrow$OF forwarding path. \\
\Exercise 
Reorder the following code snippet to minimise execution time for the following 
configurations: 

\begin{enumerate}
\item We
use software techniques, and have 2 delay slots.
\item We use interlocks, and predict not taken.
\item We use forwarding, and predict not taken.
\end{enumerate}

\begin{verbatim}
add r1, r2, r3
sub r4, r1, r1
mul r8, r9, r10
cmp r8, r9
beq .foo
\end{verbatim}

\Answer
\hspace{1mm} \\
\begin{enumerate}
\item 
\begin{verbatim}
mul r8, r9, r10
add r1, r2, r3
nop
nop
cmp r8, r9
beq .foo
nop
sub r4, r1, r1
\end{verbatim}
\item 
\begin{verbatim}
add r1, r2, r3
mul r8, r9, r10
cmp r8, r9
beq .foo
sub r4, r1, r1
\end{verbatim}
\item 
\begin{verbatim}
mul r8, r9, r10
cmp r8, r9
beq .foo
add r1, r2, r3
sub r4, r1, r1
\end{verbatim}
\end{enumerate}

\Exercise 
Reorder the following code snippet to minimise execution time for the following 
configurations: 

\begin{enumerate}
\item We
use software techniques, and have 2 delay slots.
\item We use interlocks, and predict not taken.
\item We use forwarding, and predict not taken.
\end{enumerate}

\begin{verbatim}
add r4, r3, r3
st  r3, 10[r4]
ld  r2, 10[r4]
mul r8, r9, r10
div r8, r9, r10
add r4, r2, r6
\end{verbatim}

\Answer
\hspace{1mm} \\
\begin{enumerate}
\item 
\begin{verbatim}
add r4, r3, r3
mul r8, r9, r10
nop
nop
st  r3, 10[r4]
ld  r2, 10[r4]
div r8, r9, r10
nop
nop
add r4, r2, r6
\end{verbatim}
\item 
\begin{verbatim}
add r4, r3, r3
mul r8, r9, r10
st  r3, 10[r4]
ld  r2, 10[r4]
div r8, r9, r10
add r4, r2, r6
\end{verbatim}
\item 
\begin{verbatim}
add r4, r3, r3
mul r8, r9, r10
st  r3, 10[r4]
ld  r2, 10[r4]
div r8, r9, r10
add r4, r2, r6
\end{verbatim}
\end{enumerate}

\Exercise
Answer the following:

\begin{verbatim}
add r1, r2, r3
sub r4, r1, r6
ld  r5, 10[r4]
add r6, r5, r5
sub r8, r8, r9
mul r10, r10, r11
cmp r8, r10
beq .label
add r5, r6, r8
st  r3, 20[r5]
ld  r6, 20[r5]
ld  r7, 20[r6]
lsl r7, r7, r10
\end{verbatim} 

\begin{enumerate}[i) ]

\item Assuming a traditional \simplerisc pipeline, how many cycles will this code take to execute in a pipeline
with just interlocks? 
Assume that time starts when the first instruction reaches the RW stage. 
This means that if we had just one instruction, then it would have taken  exactly 1 cycle to execute (Not 5).
Moreover, assume that the branch is not taken.
[Assumptions: No forwarding, No delayed branches, No reordering] 

\item Now, compute the number of cycles with forwarding (no delayed branches, no reordering). 

\item Compute the minimum number of cycles when we have forwarding, and we allow instruction reordering.
We do not have delayed branches, and in the reordered code, the branch instruction cannot be one of the last three
instructions.

\item Compute the minimum number of cycles when we have forwarding, allow instruction reordering, and have
delayed branches. Here, again, we are not allowed to have the branch instruction as one of the last three
instructions in the reordered code.
\end{enumerate}
\Answer
\hspace{1mm} \\
\begin{enumerate}[i) ]
\item 
Assuming a traditional \simplerisc pipeline with just interlocks and that the branch is not taken, it takes 34 cycles to execute, with the assumption that the time starts when the first instruction reaches the RW stage. 
\item  
Assuming a \simplerisc pipeline with forwarding, it takes 16 cycles to execute, with the assumption that the time starts when the first instruction reaches the RW stage.
\item 
Assuming a \simplerisc pipeline with forwarding and instruction reordering, it takes 13 cycles to execute, with the assumption that the time starts when the first instruction reaches the RW stage.
The reordered instruction set is as follows: \\
\begin{verbatim}
add r1, r2, r3
sub r4, r1, r6
ld  r5, 10[r4]
sub r8, r8, r9
add r6, r5, r5
add r5, r6, r8
st  r3, 20[r5]
ld  r6, 20[r5]
cmp r8, r10
beq .label
ld  r7, 20[r6]
mul r10, r10, r11
lsl r7, r7, r10
\end{verbatim} 
\item 
Assuming a \simplerisc pipeline with forwarding and instruction reordering and delayed branches, it takes 13 cycles to execute, with the assumption that the time starts when the first instruction reaches the RW stage.
The reordered instruction set is as follows: \\
\begin{verbatim}
add r1, r2, r3
sub r4, r1, r6
ld  r5, 10[r4]
sub r8, r8, r9
add r6, r5, r5
add r5, r6, r8
st  r3, 20[r5]
ld  r6, 20[r5]
cmp r8, r10
beq .label
ld  r7, 20[r6]
mul r10, r10, r11
lsl r7, r7, r10
\end{verbatim} 
\end{enumerate}
\Exercise[difficulty=2]
We have assumed up till now that each memory access requires one cycle. Now, let us assume that each
memory access takes two cycles. How will you modify the data path and the control path of the \simplerisc
processor in this case.

\Answer
This can be implemented by adding an extra two-cycle stage between Mem and WB stages. The instructions which do not require memory
access go through this stage, while load and store instructions go through the regular Mem stage. We also need to provide one cycle
stall between any two memory accesses.

\Exercise[difficulty=2]
Assume you have a pipeline that contains a value predictor for memory. If there is a miss in the L2
cache, then we try to predict the value and supply it to the processor. Later this value is compared
with the value obtained from memory. If the value matches, then we are fine, else we need to initiate a
process of recovery in the processor and discard all the wrong computation. Design a scheme to do this
effectively.

\Answer
We can have a fixed sized structure (a separate hardware) to store the data
values to be predicted. A hash function would map all the addresses of main
memory to a smaller address space of this value predictor. Whenever a memory
location is evicted from L2 cache to be
written back to main memory, the corresponding location of value predictor
(after calculating hash value) would also be updated with the same data value.
Whenever there is a miss in the L2 cache, the value from this predictor can be
supplied to the processor. When
the correct value has been fetched from main memory, it would be matched
against the predicted value. If the value matches, we are fine, otherwise the
value in the predictor would be updated with this new value and a recovery
would be initiated in the processor to
roll back and discard all the wrong computations.


\end{ExerciseList}

\section*{Performance and Power Modelling}

\begin{ExerciseList}
\Exercise
If we increase the average CPI (Cycles per Instruction) by 5\%, decrease the instruction count by 20\% and
double the clock rate, what is the expected speedup, if any, and why?

\Answer
Initial Total Time ($T_i) = CPI \times Instruction\,Count \times Clock\, Cycle\, Time $\\
Final Total Time ($T_f) = 1.05\:CPI \times 0.80\: Instruction\,Count \times 0.5\:Clock\, Cycle\, Time $\\
Speedup = $T_i / T_f = 2.38$


\Exercise
What should be the ideal number of pipeline stages ($x$) for a processor with
$CPI = (1+0.2x)$ and clock cycle time $t_{clk} = (1+50/x)$?

\Answer
$f(x) = CPI \times t_{clk} = (1+0.2x)\times(1+50/x) $\\
For optimum value of $f(x)$ , \\
$\frac{d}{dx}f(x) = 0\\
\Rightarrow x = 5$

\Exercise
What is the relationship between dependences in a program, and the optimal number of pipeline stages
it requires?
\Answer
For running programs with a lot of dependences, we should use processors with less number of pipeline stages. This is because, higher the number of dependences, lesser is the IPC(instructions per cycle), and we need to use pipelines with lesser number of stages. Hence, the number of dependences and the optimal number of pipeline stages are inversely related.
\Exercise
Is a 4 GHz machine faster than a 2 GHz machine? Justify your answer.
\Answer
A 4GHz machine is not necessarily faster than a 2 GHz machine. This is because, the performance of the processor does not depend on the clock frequency alone. It also depends on the IPC(Instructions per cycle), and the number of instructions which is generally assumed to be constant while comparing the relative performance of two processors. The IPC, however depends on the compiler, and the architecture which includes the number of pipeline stages. \\
\Exercise
How do the manufacturing technology, compiler, and architecture determine the
performance of a processor?
\Answer
\hspace{1mm} \\
Let us see how each of them affect the performance of the processor: \\ \\
1. \textit{Manufacturing Technology}: \\
Manufacturing technology affects the speed of transistors, and in turn the speed of combinational logic blocks, and latches. Transistors are steadily getting smaller and faster. Consequently, the total algorithmic work and latch delay are steadily reducing. Hence, it is possible to run processors at higher frequencies leading to performance improvements. Again, manufacturing technology affects only the processor frequency, and does not affect the IPC, or the number of instructions. \\ \\
2. \textit{Compiler}: \\
By using a smart compiler technology, we can reduce the number of dynamic instructions, and reduce the number of stalls. This will improve the IPC. Compilers analyse typically hundreds of instructions, and optimally reorder them to reduce stalls as much as possible. \\ \\
3.\textit{Architecture}: \\
The architecture mainly affects the IPC, the number of instructions executed per cycle. This in turn, depends on the number of pipeline stages. The main benefit of pipelining is that it allows us to run the processor at a higher frequency. In absence of stalls, the instruction throughput in a pipelined processor is quite high compared to that of a single-cycle processor. \\ 
\Exercise
Define dynamic power and leakage power.
\Answer
\hspace{1mm} \\ 
1. \textit{Dynamic power}: \\
Dynamic power is the cumulative power dissipated due to the transitions of inputs and outputs across all the transistors in a circuit. \\
2. \textit{Leakage Power}: \\
There is a minimal amount of current that flows across two terminals of a circuit element that are ideally supposed to be completely electrically isolated from each other. This is known as the \textit{leakage current}. When a leakage current flows across a resistive element, it dissipates power. This power is known as \textit{leakage power}. \textit{Leakage power} is static in nature and is dissipated all the time irrespective of the level of activity in a circuit. \\
\Exercise[difficulty=1]
We claim that if we increase the frequency, the leakage power increases. Justify this statement.
\Answer
According to the DVFS technique, to scale up the frequency by a factor of $k_1$, we scale up the voltage by a factor of $k_2$. Typically, we assume that $k_1$=$k_2$. Also, leakage power is proportional to square of temperature. Beyond the threshold voltage, a small increase in temperature translates to a large increase in leakage power. On increasing the frequency, we increase the voltage, leading to an increase in temperature. This leads to increase in leakage power.
\Exercise
What is the justification of the $ED^2$ metric?
\Answer
Let us compare two processor designs for the same program. One design dissipates $E_1$ joules and takes $D_1$ units of time, whereas the other design dissipates $E_2$ joules and takes $D_2$ units of time. To compare the designs, we need a common metric. Without loss of generality, we assume that D$\propto$1/f . \\
We need to either make the performance same, ie: $D_1$=$D_2$ and compare the energy, or make energy same ie: $E_1$=$E_2$ , and compare performance. To ensure $D_1$=$D_2$, we need to speed up one design or slowdown the other. We use DVFS technique for the same. \\
According to the DVFS technique, to scale up the frequency by a factor of $k_1$, we scale up the voltage by a factor of $k_2$. Typically, $k_1$=$k_2$. \\
To equalise the execution times of designs 1 and 2, we use the following assumptions. D$\propto$1/f, f$\propto$V. Thus, D$\propto$1/V. We scale the delay of design 2 by $D_1$/$D_2$, or alternatively scale its voltage and frequency by $D_2$/$D_1$. Let the energy dissipation of design 2 now be $E_2^{'}$. Since E$\propto$$V^{2}$, we have \\
$E_2^{'}$= $E_2$X$\frac{V_1^{2}}{V_2^{2}}$ \\
          = $E_2$X$\frac{f_1^{2}}{f_2^{2}}$ \\
 = $E_2$X$\frac{D_2^{2}}{D_1^{2}}$ \\
Let us compare $E_1$ and $E_2^{'}$, \\
$E_2^{'}$$\Longleftrightarrow$$E_1$ \\
$E_2^{'}$X$\frac{D_2^{2}}{D_1^{2}}$$\Longleftrightarrow$$E_1$ \\
$E_2^{'}D_2^{2}$$\Longleftrightarrow$$E_1D_1^{2}$ \\ \\
Hence, $ED^{2}$=k, where $k$ is the constant of proportionality. This is a property that is independent of voltage and frequency of the system. This is used as an effective baseline metric to compare two designs. \\  
\Exercise[difficulty=1]
How do power and temperature considerations limit the number of pipeline stages? Explain your answer
in detail. Consider all the relationships between power, temperature, activity, IPC, and frequency
that we have introduced in this chapter.
\Answer
High performance processor chips typically dissipate 60-120 W during normal operation. If we have four chips in a server class computer, then we roughly dissipate 400 W of power. The rest of the components in a computer such as main memory, hard disk, peripherals etc. dissipate a similar amount of power. Thus, total power consumption comes to 800 W. With additional overheads, this may increase upto 1kW. A small server farm containing 100 machines will dissipate 100 kW. To remove 1W of heat, we need 0.5 W of heat approximately. For larger server farms containing 1000 machines, we require megawatts of power which equals the power requirements of a city. \\
Power dissipation in small devices again is crucial due to limited amount of battery life. Thus, power dissipation has to be minimised.\\
Coming to temperature, it increases with an increase in power dissipation. Increased temperature has a lot of deleterious effects. The reliability of  on-chip copper wires, and transistors decreases exponentially with increasing temperature. Due to NBTI( Negative Bias Temperature Instability), chips tend to age over time. This slows down transistors. Hence , the frequency of processor chips has to be reduced over time to ensure correct operation to reduce the large increase in temperature, and increased power dissipation. \\
This reduction in frequency is directly related to the number of pipeline stages. Less frequency implies greater time period, which means the amount of work per stage is increased. This is a direct result of limited number of pipeline stages, and hence, decreased instruction throughput. Also, with a decrease in power dissipation, the $Delay$ is increased according to the $ED^{2}$ metric, and this again leads to a decreased frequency, and a decreased number of pipeline stages. \\
\Exercise[difficulty=1]
Define the term, DVFS?
\Answer
DVFS stands for dynamic voltage-frequency scaling.According to the DVFS technique, to scale up the frequency by a factor of $k_1$, we scale up the voltage by a factor of $k_2$. Typically, we assume that $k_1$=$k_2$. Note that with a higher frequency and consequent lower clock cycle time, we need to ensure that signals can rise and fall quickly. To ensure quicker signal transition, we increase the voltage such that it takes lesser amount of time for a signal to rise and fall by $\delta$V volts. Hence, DVFS is a  technique used to adjust the voltage and frequency of a processor at run time.  
\Exercise[difficulty=2]
Assume that we wish to estimate the temperature at different points of a processor. We know the
dynamic power of different components, and the leakage power as a function of temperature. Furthermore,
we divide the surface of the die into a grid as explained in Section~\ref{sec:temperature}. How do we
use this information to arrive at a steady state value of temperature for all the grid points?

\end{ExerciseList}

\section*{Interrupts and Exceptions}

\begin{ExerciseList}

\Exercise
What do you mean by precise exceptions? How does hardware ensure that every exception is a precise exception?

\Answer
A precise exception is an exception which always comes at instruction boundary.
It means that all instructions up to the excepting instruction have been executed, 
and the excepting instruction and everything afterwards have not been executed.

Since hardware exceptions (or interrupts) can come asynchronously, hardware pushes them into a buffer,
and the interrupt is given to the processor only at the processor boundary. Processor then completes the
instruction at Write-Back stage, and the instructions up to Mem stage are discarded.

\Exercise
Why do we need the $movz$ and $retz$ instructions?
\Answer
We add a few special registers to the pipelined processor for interrupt handling, namely \textit{oldPC,oldSP,oldFlags,flags}. Next, we add a set of privileged instructions that are accesible only to specialised programs such as operating systems, and interrupt handlers. They have more visibility to the internals of the processor. Since they are very powerful, it is not a good idea to give the programmers the ability to invoke them. They might corrupt the program state, and introduce viruses. \\
Thus, we have the $movz$ instruction that transfers the values between the regular registers and the above mentioned special registers. The $retz$ instruction reads the value of $oldPC$, and transfers its contents to PC. 
\Exercise
List the additional registers that we add to a pipeline to support interrupts and exceptions.
\Answer
The additional special set of registers that we add to the pipeline to support interrupts and exceptions are - \textit{oldPC,oldSP,oldFlags} and $flags$. 
\Exercise
What is the role of the $CPL$ register? How do we set and reset it?
\Answer
The \textit{privileged instructions} are typically used by interrupt handlers, and other modules of the operating system.They have more visibility to the internals of the processor. Since they are very powerful, it is not a good idea to give the programmers the ability to invoke them. They might corrupt the program state, and introduce viruses. Hence, most systems typically disallow the usage of privileged instructions by normal programs. Most processors thus, use a register that contains the \textit{current privilege level}. It is typically 1 for user programs, and 0 for operating system programs. There is a privilege level change, when we switch to processing an interrupt handler(1 to 0), and when we execute the $retz$ instruction to return to user program (0 to 1). When a privileged instruction is executed, the processor checks the CPL register, and if it not allowed to execute the instruction, then the exception is flagged.  
\Exercise
How do we locate the correct interrupt handler? What is structure and role of an interrupt
handler?
\Answer
For exception and interrupt handling, we add an \textit{exception unit} to our pipeline. Its role is to process interrupts and exceptions.\\
To properly take care of exceptions, we need to mark an instruction immediately after it causes an exception. Once it is marked, we need to let the exception unit know. Secondly, we envision a small circuit that sends a code identifying the exception/interrupt to the exception unit. Subsequently, the exception unit needs to wait for the marked instruction to reach the end of the pipeline such that all the instructions before the marked instruction complete their execution. Instructions after this are converted to bubbles. Once, the marked instruction reaches the end of the pipeline, the exception unit loads the PC with the starting address of the interrupt handler. The interrupt handler can then begin execution. Hence, the interrupt handler ensures that all exception/interrupts remain precise.
\Exercise
Why do we need the registers $oldPC$, and $oldSP$?
\Answer
We need a mechanism to save and restore the state of the program where we left off to execute the interrupt handler, to return to the correct point in the program. For this purpose, we use a set of special registers namely \textit{oldPC,oldSP,oldFlags,flags} to store the program state and context. An $N$ PC field for the next PC is added to the instruction packet. By default, it is equal to PC+4. However, for branch instructions, it is equal to the branch target. In the EX stage, a circuit is used for adding either the $branchTarget$ or the $PC+4$ value to the $N$ field. Once, the marked instruction reaches RW stage, the exception unit looks up a small internal table indexed by the interrupt/exception code. For certain types of instructions, the PC of the marked instruction has to be returned (eg .page faults), but for certain other types, the PC of the next instruction has to be returned (eg. I/O interrupts). The exception unit thus uses the $oldPC$ register and transfers the correct return address to it. \\
We also need a mechanism to save and restore registers. Interrupt handlers have their own stacks that are resident in their private memory regions. To use the stack pointer of an interrupt handler, we need to load it to $sp$. This will overwrite the previous value of $sp$, which is the value of the stack pointer of the program. Hence, we use another register called $oldSP$. The interrupt handler first transfers the contents of $sp$ to $oldSP$. Then, it loads $sp$ with the stack pointer of interrupt handler, and spills all the register contents. At the end, it transfers back the contents of $oldSP$ to the stack. 
\Exercise
Why do we need to add a $flags$ field to the instruction packet? How do we use
the $oldFlags$ register?
\Answer
We need to save the contents of the $flags$ register, as it is also required to be saved and restored as a part of the program context. For this, we first add the $flags$ field to the instruction packet. Instructions other than $cmp$ write the contents of the $flags$ register to the $flags$ field in the instruction packet in the EX stage. The $cmp$ instruction writes the updated value of the $flags$ register to the $flags$ field in the EX stage. When a marked instruction reaches the RW stage, the exception unit extracts the contents of $flags$ field, and saves it to the $oldFlags$ register. This is visible only to the ISA, and helps store the last value of the $flags$ register that a valid instruction had seen. 
\Exercise[difficulty=1]
Consider a hypothetical situation where a write back to a register may generate an
exception (register-fault exception). Propose a mechanism to handle this exception
{\em precisely}.

\Answer
Whenever an exception comes, check if it register-fault exception. If it is,
save the PC of the instruction in Write Back stage (as well as other
registers), and proceed as earlier. If it is some other instruction, save the
PC of instruction in Mem stage.
A more efficient mechanism could involve an extra register, which would store
the value to be written to the register in write-back stage (we may require
another register to store the register address). Whenever a register-fault
exception comes, the exception 
handler can resolve it and then write back the values in this new register
while restoring other registers. Here we would need to save the PC of
instruction in Mem stage. 

\Exercise[difficulty=1]
Define the concept of register windows? How can we use register windows to speedup
the implementation of functions?
\Answer
A register window is defined as a set of registers that a particular instruction or function can access. For example, in the \simplerisc pipeline, the privileged instructions can access only six registers, out of which four are special registers. In comparison, the regular instructions have a register window that contains 16 general purpose registers, but no special registers.
The register window concept is mainly used to avoid costly register spills. Here, it is used to separate the set of registers that can be accessed by user programs and interrupt handlers. \\
Different parts of a computer program all use their own temporary values, and therefore compete for the use of the registers. Since a good understanding of the nature of program flow at runtime is very difficult, there is no easy way for the developer to know in advance how many registers they should use, and how many to leave aside for other parts of the program. In general these sorts of considerations are ignored, and the developers, and more likely, the compilers they use, attempt to use all the registers visible to them. In the case of processors with very few registers to begin with, this is also the only reasonable course of action. \\
Register windows aim to solve this issue. Since every part of a program wants registers for its own use, several sets of registers are provided for the different parts of the program. If these registers were visible, there would be more registers to compete over, i.e. they have to be made invisible.Rendering the registers invisible can be implemented efficiently; the CPU recognizes the movement from one part of the program to another during a procedure call. It is accomplished by one of a small number of instructions (prologue) and ends with one of a similarly small set (epilogue). Thus, this helps in speeding up the implementation of procedure calls.
\end{ExerciseList}

\section*{Advanced Topics}

\begin{ExerciseList}
\Exercise
Can you intuitively say why most of the branches in programs are predictable?
\Answer
Most of the branches in programs are predictable. This is because most branches are typically found in loops,or in $if$ statements where both the directions are not equally likely. In most cases, one direction is more likely than the other. Branches in loops are taken most of the times except for the last iteration when the loop is exited. Similarly, we have $if$ statements that are executed only if an exceptional condition is true. In such cases, most of the time, the $if$ branch is not taken. In this way, for most programs, processor designers have observed that almost all the branch instructions follow certain patterns. They either have a strong bias towards one direction, or can be predicted on the basis of past history, or even based on the behaviour of other branches. \\ 
\Exercise
Is the following code sequence amenable to branch prediction. Why or why not?
\begin{Verbatim}[frame=single]
int status=flip_random_unbiased_coin();
if (status==Head)
	print(“head”);
else
	print(“tail”);
\end{Verbatim}
\Answer
The code is not amenable to branch prediction.
Because of the random nature of the code, any branch prediction algorithm will have
a probability of 0.5 for correct prediction. 


\Exercise
We need to design a 2-issue inorder pipeline that accepts a bundle of two instructions
every cycle. These bundles are created by the compiler.
\begin{enumerate}[(a) ]
\item
Given the different instruction types, design an algorithm that tells
the compiler the different constraints in designing a bundle. For example, you might
decide that you don't want to have two instructions in a bundle if they 
are of certain types, or have certain operands.

\item To implement a two issue pipeline, what kind of additional functionality will you need in the MEM stage?
\end{enumerate}

\Answer:

\begin{enumerate}[(a) ]
 \item The following care has to be taken in bundling two instructions:
 \begin{itemize}
  \item If the instructions write to same destination (register or memory
location), ignore the first write and commit the second write.  \item If
destination register of 1st instruction is the source register of 2nd
instruction, stall the 2nd instruction until the 1st 
  instruction reaches write back stage, and then forward the value. We may also
choose not to bundle this kind of instructions.  \item If a store instruction
is followed by load instruction reading data from the same memory location, the
data can be forwarded
  to load instruction.
  \item If a load is followed by store at the same memory location, stall the store
instruction until load completes.  \end{itemize}
 \item Additional hardware support is needed to enable 2 concurrent writes, 2
concurrent reads and a read and a write to the disk.  Also, a check has to be
performed is the bindle contains 2 load instructions and the source memory
location is same; in which case,
 a single read should feed the values to both the instructions.
\end{enumerate}

\Exercise
Describe the main insight behind out-of-order pipelines? What are their major structures?
\Answer
In-order pipelines execute instructions in the order in which they appear. In case of data dependencies between adjacent instructions, the pipeline has to stall. However, if we can execute them in an order that is not consistent with the program order, we can do better with the performance. \\
An out-of-order pipeline fetches instructions in-order. After the fetch stage, it proceeds to decode the instructions. These are simultaneously added to a queue called the \textit{reorder buffer} (ROB) in program order. After decoding the instruction, \textit{register renaming} is carried out. Apart from the architectural registers, most modern processor use \textit{physical registers} which are visible internally. These are used for overcoming the \textit{WAW} ( Write after Write) and \textit{WAR}(Write After Read) hazards. \\
The only ones that remain are \textit{RAW} (Read After Write) dependences. The instructions after renaming enter the \textit{instruction window}, which monitors the source operands. The instruction is issued to the corresponding functional unit once all the source operands are ready. Then, the results are broadcasted to all instructions in the \textit{instruction window}. Instructions, waiting for the results, mark their source operands ready. This is called \textit{instruction wakeup}. To avoid structural hazards, an \textit{instruction select} unit chooses a set of instructions for execution. There is also a \textit{load-store} queue which saves the list of loads and stores in program order. \\
After an instruction completes its execution, we mark its entry in the \textit{reorder buffer}. Instructions enter and leave the \textit{reorder buffer} in program order to ensure exceptions are precise. \\
Hence, out-of-order pipelines can execute instructions that do not have RAW dependences, in parallel. Most programs have such sets of instructions at most points of time. This is known as Instruction level parallelism.\\  
\end{ExerciseList}

\section*{Design Problems}

\begin{ExerciseList}
\Exercise
Implement a basic pipelined processor with interlocks using Logisim (refer to the design problems
in Chapter~\ref{chap:impl}).

\Exercise
Implement a basic pipelined processor in a hardware description such as Verilog or VHDL. Try to
add forwarding paths, and interrupt processing logic.

\Exercise
Learn the language SystemC. It is used to model hardware at a high level. Implement the \simplerisc
pipeline in SystemC. 

\end{ExerciseList}
